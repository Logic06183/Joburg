{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_data_files():\n",
    "    \"\"\"Verify all required data files are present.\"\"\"\n",
    "    required_files = [\n",
    "        'Train.csv',\n",
    "        'Test.csv',\n",
    "        'composite_images.npz',\n",
    "        'SampleSubmission.csv'\n",
    "    ]\n",
    "    \n",
    "    missing_files = []\n",
    "    for file in required_files:\n",
    "        if not os.path.exists(os.path.join(BASE_PATH, file)):\n",
    "            missing_files.append(file)\n",
    "    \n",
    "    if missing_files:\n",
    "        raise FileNotFoundError(\n",
    "            f\"The following files are missing in {BASE_PATH}: {', '.join(missing_files)}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing datasets...\n",
      "Dataset shapes:\n",
      "Train: (606, 730, 15), (606, 128, 128, 6)\n",
      "Valid: (68, 730, 15), (68, 128, 128, 6)\n",
      "Test: (224, 730, 15), (224, 128, 128, 6)\n"
     ]
    }
   ],
   "source": [
    "# Improved Flood Detection Model for South Africa\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Tuple, Optional\n",
    "from collections.abc import Callable, Sequence\n",
    "from functools import partial\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Deep Learning\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    \"\"\"Model and training configuration.\"\"\"\n",
    "    # Paths\n",
    "    BASE_PATH = Path('c:/Users/CraigParker/OneDrive - Wits Health Consortium/PHR PC/Downloads/Joburg/Zindi_comp')\n",
    "    \n",
    "    # Data\n",
    "    SEED = 42\n",
    "    VALID_SIZE = 0.1\n",
    "    BAND_NAMES = ('B2', 'B3', 'B4', 'B8', 'B11', 'slope')\n",
    "    IMG_SHAPE = (128, 128, len(BAND_NAMES))\n",
    "    \n",
    "    # Model\n",
    "    HIDDEN_DIM = 256\n",
    "    NUM_HEADS = 4\n",
    "    NUM_LAYERS = 6\n",
    "    DROPOUT = 0.1\n",
    "    \n",
    "    # Training\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 100\n",
    "    LEARNING_RATE = 1e-4\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    \n",
    "    # Time series preprocessing\n",
    "    WINDOW_SIZES = [3, 7, 14, 30]\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"Handles data loading and preprocessing.\"\"\"\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.rng = np.random.default_rng(config.SEED)\n",
    "    \n",
    "    def load_raw_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, Dict]:\n",
    "        \"\"\"Load raw data files.\"\"\"\n",
    "        # Load CSV files\n",
    "        train_df = pd.read_csv(self.config.BASE_PATH / 'Train.csv')\n",
    "        test_df = pd.read_csv(self.config.BASE_PATH / 'Test.csv')\n",
    "        \n",
    "        # Load image data\n",
    "        images = np.load(self.config.BASE_PATH / 'composite_images.npz')\n",
    "        \n",
    "        return train_df, test_df, images\n",
    "    \n",
    "    def preprocess_time_series(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Process time series data with rolling features.\"\"\"\n",
    "        # Ensure 2D array\n",
    "        if data.ndim == 1:\n",
    "            data = data[np.newaxis, :]\n",
    "            \n",
    "        features = []\n",
    "        # Original data\n",
    "        features.append(data)\n",
    "        \n",
    "        # Rolling statistics\n",
    "        for window in self.config.WINDOW_SIZES:\n",
    "            # Rolling mean\n",
    "            rolling_mean = np.array([\n",
    "                np.convolve(row, np.ones(window)/window, mode='same')\n",
    "                for row in data\n",
    "            ])\n",
    "            features.append(rolling_mean)\n",
    "            \n",
    "            # Rolling max\n",
    "            rolling_max = np.array([\n",
    "                np.maximum.accumulate(row)\n",
    "                for row in rolling_mean\n",
    "            ])\n",
    "            features.append(rolling_max)\n",
    "            \n",
    "            # Rolling std\n",
    "            rolling_std = np.array([\n",
    "                [np.std(row[max(0, i-window):i+1]) for i in range(len(row))]\n",
    "                for row in data\n",
    "            ])\n",
    "            features.append(rolling_std)\n",
    "        \n",
    "        # Cumulative sum\n",
    "        features.append(np.cumsum(data, axis=1))\n",
    "        \n",
    "        # Smoothed signal\n",
    "        features.append(gaussian_filter1d(data, sigma=2.0, axis=1))\n",
    "        \n",
    "        # Combine features\n",
    "        combined = np.stack(features, axis=-1)\n",
    "        \n",
    "        # Normalize\n",
    "        mean = np.mean(combined, axis=(0, 1), keepdims=True)\n",
    "        std = np.std(combined, axis=(0, 1), keepdims=True) + 1e-8\n",
    "        return ((combined - mean) / std).astype(np.float32)\n",
    "    \n",
    "    def preprocess_image(self, image: np.ndarray, augment: bool = False) -> np.ndarray:\n",
    "        \"\"\"Process satellite imagery.\"\"\"\n",
    "        # Split bands\n",
    "        spectral = image[..., :-1].astype(np.float32)\n",
    "        slope = image[..., -1:].astype(np.float32)\n",
    "        \n",
    "        # Normalize spectral bands\n",
    "        spectral = (spectral - 1250) / 500\n",
    "        \n",
    "        # Convert slope to radians\n",
    "        slope = (slope / np.iinfo(np.uint16).max * (np.pi / 2.0))\n",
    "        \n",
    "        # Combine processed bands\n",
    "        processed = np.concatenate([spectral, slope], axis=-1)\n",
    "        \n",
    "        if augment and self.rng.random() > 0.5:\n",
    "            # Random flip\n",
    "            if self.rng.random() > 0.5:\n",
    "                processed = np.flip(processed, axis=0)\n",
    "            if self.rng.random() > 0.5:\n",
    "                processed = np.flip(processed, axis=1)\n",
    "            # Random 90-degree rotation\n",
    "            k = self.rng.integers(4)\n",
    "            processed = np.rot90(processed, k=k)\n",
    "            \n",
    "        return processed\n",
    "    \n",
    "    def prepare_datasets(self) -> Tuple[Dict, Dict, Dict]:\n",
    "        \"\"\"Prepare train, validation and test datasets.\"\"\"\n",
    "        # Load data\n",
    "        train_df, test_df, images = self.load_raw_data()\n",
    "        \n",
    "        # Process event IDs\n",
    "        train_df['event_id'] = train_df['event_id'].apply(lambda x: '_'.join(x.split('_')[:2]))\n",
    "        test_df['event_id'] = test_df['event_id'].apply(lambda x: '_'.join(x.split('_')[:2]))\n",
    "        \n",
    "        # Ensure required columns are present\n",
    "        required_columns = {'event_id', 'precipitation'}\n",
    "        if not required_columns.issubset(train_df.columns):\n",
    "            missing_cols = required_columns - set(train_df.columns)\n",
    "            raise KeyError(f\"Missing columns in train_df: {missing_cols}\")\n",
    "        if not required_columns.issubset(test_df.columns):\n",
    "            missing_cols = required_columns - set(test_df.columns)\n",
    "            raise KeyError(f\"Missing columns in test_df: {missing_cols}\")\n",
    "        \n",
    "        # Split train/validation\n",
    "        train_events, valid_events = train_test_split(\n",
    "            train_df['event_id'].unique(),\n",
    "            test_size=self.config.VALID_SIZE,\n",
    "            random_state=self.config.SEED\n",
    "        )\n",
    "        \n",
    "        # Prepare datasets\n",
    "        def prepare_set(events, df):\n",
    "            mask = df['event_id'].isin(events)\n",
    "            subset = df[mask]\n",
    "            \n",
    "            # Time series data\n",
    "            ts = subset.pivot_table(\n",
    "                index='event_id',\n",
    "                columns=subset.groupby('event_id').cumcount(),\n",
    "                values='precipitation'\n",
    "            ).to_numpy()\n",
    "            \n",
    "            # Images\n",
    "            imgs = np.stack([images[eid] for eid in events])\n",
    "            \n",
    "            # Labels (if available)\n",
    "            labels = None\n",
    "            if 'label' in subset.columns and 'event_t' in subset.columns:\n",
    "                labels = subset.pivot(\n",
    "                    index='event_id',\n",
    "                    columns='event_t',\n",
    "                    values='label'\n",
    "                ).to_numpy()\n",
    "            \n",
    "            return {\n",
    "                'timeseries': self.preprocess_time_series(ts),\n",
    "                'images': imgs,\n",
    "                'labels': labels\n",
    "            }\n",
    "        \n",
    "        train_data = prepare_set(train_events, train_df)\n",
    "        valid_data = prepare_set(valid_events, train_df)\n",
    "        test_data = prepare_set(test_df['event_id'].unique(), test_df)\n",
    "        \n",
    "        return train_data, valid_data, test_data\n",
    "\n",
    "def create_model(config: Config):\n",
    "    \"\"\"Create the flood detection model.\"\"\"\n",
    "    # Model architecture code will go here\n",
    "    pass\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    config = Config()\n",
    "    \n",
    "    # Initialize data processor\n",
    "    processor = DataProcessor(config)\n",
    "    \n",
    "    print(\"Preparing datasets...\")\n",
    "    train_data, valid_data, test_data = processor.prepare_datasets()\n",
    "    \n",
    "    print(\"Dataset shapes:\")\n",
    "    print(f\"Train: {train_data['timeseries'].shape}, {train_data['images'].shape}\")\n",
    "    print(f\"Valid: {valid_data['timeseries'].shape}, {valid_data['images'].shape}\")\n",
    "    print(f\"Test: {test_data['timeseries'].shape}, {test_data['images'].shape}\")\n",
    "    \n",
    "    # Model training code will go here\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2070608978.py, line 376)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[21], line 376\u001b[1;36m\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Tuple\n",
    "import json\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Deep Learning\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Paths\n",
    "        self.base_path = Path(r'C:\\Users\\CraigParker\\OneDrive - Wits Health Consortium\\PHR PC\\Downloads\\Joburg\\Zindi_comp')\n",
    "        \n",
    "        # Data\n",
    "        self.seed = 42\n",
    "        self.valid_size = 0.1\n",
    "        self.band_names = ('B2', 'B3', 'B4', 'B8', 'B11', 'slope')\n",
    "        \n",
    "        # Model\n",
    "        self.patch_size = 16\n",
    "        self.hidden_dim = 512\n",
    "        self.num_layers = 8\n",
    "        self.num_heads = 8\n",
    "        self.dropout = 0.1\n",
    "        \n",
    "        # Training\n",
    "        self.batch_size = 32\n",
    "        self.num_epochs = 100\n",
    "        self.learning_rate = 1e-4\n",
    "        self.weight_decay = 0.01\n",
    "        \n",
    "        # Data processing\n",
    "        self.required_files = ['Train.csv', 'Test.csv', 'composite_images.npz', 'SampleSubmission.csv']\n",
    "        self.img_shape = (128, 128, len(self.band_names))\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.rng = np.random.default_rng(config.seed)\n",
    "    \n",
    "    def load_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, Dict]:\n",
    "        \"\"\"Load and preprocess all data.\"\"\"\n",
    "        # Load CSV files\n",
    "        data = pd.read_csv(self.config.base_path / 'Train.csv')\n",
    "        data_test = pd.read_csv(self.config.base_path / 'Test.csv')\n",
    "        \n",
    "        # Process event IDs and create time steps\n",
    "        for df in [data, data_test]:\n",
    "            df['event_id'] = df['event_id'].apply(lambda x: '_'.join(x.split('_')[:2]))\n",
    "            df['event_t'] = df.groupby('event_id').cumcount()\n",
    "        \n",
    "        # Load images\n",
    "        images = np.load(self.config.base_path / 'composite_images.npz')\n",
    "        \n",
    "        return data, data_test, images\n",
    "    \n",
    "    def preprocess_timeseries(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Process time series data.\"\"\"\n",
    "        features = [data]\n",
    "        \n",
    "        # Add rolling statistics\n",
    "        windows = [3, 7, 14, 30]\n",
    "        for window in windows:\n",
    "            # Mean\n",
    "            rolling_mean = np.array([\n",
    "                np.convolve(row, np.ones(window)/window, mode='same')\n",
    "                for row in data\n",
    "            ])\n",
    "            features.append(rolling_mean)\n",
    "            \n",
    "            # Max\n",
    "            rolling_max = np.array([\n",
    "                np.maximum.accumulate(row) for row in rolling_mean\n",
    "            ])\n",
    "            features.append(rolling_max)\n",
    "        \n",
    "        # Add cumulative and smoothed\n",
    "        features.append(np.cumsum(data, axis=1))\n",
    "        features.append(gaussian_filter1d(data, sigma=2.0, axis=1))\n",
    "        \n",
    "        # Combine and normalize\n",
    "        combined = np.stack(features, axis=-1)\n",
    "        mean = np.mean(combined, axis=(0, 1), keepdims=True)\n",
    "        std = np.std(combined, axis=(0, 1), keepdims=True) + 1e-8\n",
    "        return ((combined - mean) / std).astype(np.float32)\n",
    "    \n",
    "    def preprocess_image(self, image: np.ndarray, augment: bool = False) -> np.ndarray:\n",
    "        \"\"\"Process satellite imagery.\"\"\"\n",
    "        # Split bands\n",
    "        spectral = image[..., :-1].astype(np.float32)\n",
    "        slope = image[..., -1:].astype(np.float32)\n",
    "        \n",
    "        # Normalize\n",
    "        spectral = (spectral - 1250) / 500\n",
    "        slope = slope / np.iinfo(np.uint16).max * (np.pi / 2.0)\n",
    "        \n",
    "        # Combine\n",
    "        processed = np.concatenate([spectral, slope], axis=-1)\n",
    "        \n",
    "        if augment and self.rng.random() > 0.5:\n",
    "            if self.rng.random() > 0.5:\n",
    "                processed = np.flip(processed, axis=0)\n",
    "            if self.rng.random() > 0.5:\n",
    "                processed = np.flip(processed, axis=1)\n",
    "            processed = np.rot90(processed, k=self.rng.integers(4))\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def prepare_datasets(self) -> Tuple[Dict, Dict, Dict]:\n",
    "        \"\"\"Prepare complete datasets for training.\"\"\"\n",
    "        # Load data\n",
    "        data, data_test, images = self.load_data()\n",
    "        \n",
    "        # Create train/validation split\n",
    "        train_events, valid_events = train_test_split(\n",
    "            data['event_id'].unique(),\n",
    "            test_size=self.config.valid_size,\n",
    "            random_state=self.config.seed\n",
    "        )\n",
    "        \n",
    "        def prepare_set(events, df):\n",
    "            subset = df[df['event_id'].isin(events)]\n",
    "            \n",
    "            # Time series\n",
    "            ts = subset.pivot(\n",
    "                index='event_id',\n",
    "                columns='event_t',\n",
    "                values='precipitation'\n",
    "            ).fillna(0).to_numpy()\n",
    "            \n",
    "            # Images\n",
    "            imgs = np.stack([images[eid] for eid in events])\n",
    "            \n",
    "            # Labels if available\n",
    "            labels = None\n",
    "            if 'label' in df.columns:\n",
    "                labels = subset.pivot(\n",
    "                    index='event_id',\n",
    "                    columns='event_t',\n",
    "                    values='label'\n",
    "                ).fillna(0).to_numpy()\n",
    "            \n",
    "            return {\n",
    "                'timeseries': self.preprocess_timeseries(ts),\n",
    "                'images': imgs,\n",
    "                'labels': labels\n",
    "            }\n",
    "        \n",
    "        return (\n",
    "            prepare_set(train_events, data),\n",
    "            prepare_set(valid_events, data),\n",
    "            prepare_set(data_test['event_id'].unique(), data_test)\n",
    "        )\n",
    "\n",
    "class FloodDetectionModel(nn.Module):\n",
    "    \"\"\"Combined model for flood detection.\"\"\"\n",
    "    config: Config\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, inputs, training: bool = True):\n",
    "        timeseries, images = inputs\n",
    "        B, T, F = timeseries.shape\n",
    "        \n",
    "        # Process time series\n",
    "        x_ts = nn.Dense(self.config.hidden_dim)(timeseries)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        position = jnp.arange(T)[None, :, None]\n",
    "        div_term = jnp.exp(\n",
    "            jnp.arange(0, self.config.hidden_dim, 2) * \n",
    "            (-math.log(10000.0) / self.config.hidden_dim)\n",
    "        )\n",
    "        pos_enc = jnp.zeros((1, T, self.config.hidden_dim))\n",
    "        pos_enc = pos_enc.at[:, :, 0::2].set(jnp.sin(position * div_term))\n",
    "        pos_enc = pos_enc.at[:, :, 1::2].set(jnp.cos(position * div_term))\n",
    "        x_ts = x_ts + pos_enc\n",
    "        \n",
    "        # Transformer layers for time series\n",
    "        for _ in range(self.config.num_layers):\n",
    "            y = nn.LayerNorm()(x_ts)\n",
    "            y = nn.MultiHeadDotProductAttention(\n",
    "                num_heads=self.config.num_heads\n",
    "            )(y, y, deterministic=not training)\n",
    "            x_ts = x_ts + y\n",
    "        \n",
    "        # Process images\n",
    "        x_img = nn.Conv(\n",
    "            features=self.config.hidden_dim,\n",
    "            kernel_size=(self.config.patch_size, self.config.patch_size),\n",
    "            strides=(self.config.patch_size, self.config.patch_size)\n",
    "        )(images)\n",
    "        \n",
    "        num_patches = (self.config.img_shape[0] // self.config.patch_size) ** 2\n",
    "        x_img = x_img.reshape(B, num_patches, self.config.hidden_dim)\n",
    "        \n",
    "        # Transformer layers for images\n",
    "        for _ in range(self.config.num_layers):\n",
    "            y = nn.LayerNorm()(x_img)\n",
    "            y = nn.MultiHeadDotProductAttention(\n",
    "                num_heads=self.config.num_heads\n",
    "            )(y, y, deterministic=not training)\n",
    "            x_img = x_img + y\n",
    "        \n",
    "        # Global average pooling for images\n",
    "        x_img = jnp.mean(x_img, axis=1)\n",
    "        \n",
    "        # Combine features\n",
    "        x_img = jnp.expand_dims(x_img, axis=1)\n",
    "        x_img = jnp.tile(x_img, (1, T, 1))\n",
    "        \n",
    "        x = jnp.concatenate([x_ts, x_img], axis=-1)\n",
    "        \n",
    "        # Output projection\n",
    "        x = nn.Dense(1)(x)\n",
    "        return jnp.squeeze(x, axis=-1)\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, config: Config, model: nn.Module, train_data: Dict, valid_data: Dict):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.train_data = train_data\n",
    "        self.valid_data = valid_data\n",
    "        \n",
    "        # Initialize training state\n",
    "        rng = jax.random.PRNGKey(config.seed)\n",
    "        dummy_batch = (\n",
    "            jnp.ones((1, 730, train_data['timeseries'].shape[-1])),\n",
    "            jnp.ones((1,) + config.img_shape)\n",
    "        )\n",
    "        variables = model.init(rng, dummy_batch)\n",
    "        \n",
    "        # Create optimizer\n",
    "        tx = optax.chain(\n",
    "            optax.clip_by_global_norm(1.0),\n",
    "            optax.adamw(\n",
    "                learning_rate=optax.cosine_decay_schedule(\n",
    "                    init_value=config.learning_rate,\n",
    "                    decay_steps=config.num_epochs,\n",
    "                    alpha=0.1\n",
    "                ),\n",
    "                weight_decay=config.weight_decay\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.state = train_state.TrainState.create(\n",
    "            apply_fn=model.apply,\n",
    "            params=variables['params'],\n",
    "            tx=tx\n",
    "        )\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Training loop with validation.\"\"\"\n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            # Training\n",
    "            with tqdm(range(0, len(self.train_data['timeseries']), self.config.batch_size),\n",
    "                     desc=f\"Epoch {epoch+1}/{self.config.num_epochs}\") as pbar:\n",
    "                \n",
    "                for i in pbar:\n",
    "                    batch_idx = slice(i, i + self.config.batch_size)\n",
    "                    batch = {\n",
    "                        'timeseries': self.train_data['timeseries'][batch_idx],\n",
    "                        'images': self.train_data['images'][batch_idx],\n",
    "                        'labels': self.train_data['labels'][batch_idx]\n",
    "                    }\n",
    "                    \n",
    "                    # Training step\n",
    "                    self.state, metrics = self.train_step(batch)\n",
    "                    pbar.set_postfix({'loss': f\"{metrics['loss']:.4f}\",\n",
    "                                    'acc': f\"{metrics['accuracy']:.4f}\"})\n",
    "            \n",
    "            # Validation\n",
    "            valid_metrics = self.evaluate()\n",
    "            print(f\"\\nValidation - Loss: {valid_metrics['loss']:.4f}, \"\n",
    "                  f\"Acc: {valid_metrics['accuracy']:.4f}\")\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def train_step(self, batch):\n",
    "        \"\"\"Single training step.\"\"\"\n",
    "        def loss_fn(params):\n",
    "            logits = self.state.apply_fn(\n",
    "                {'params': params},\n",
    "                (batch['timeseries'], batch['images'])\n",
    "            )\n",
    "            loss = optax.sigmoid_binary_cross_entropy(logits, batch['labels']).mean()\n",
    "            return loss, logits\n",
    "        \n",
    "        (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(self.state.params)\n",
    "        state = self.state.apply_gradients(grads=grads)\n",
    "        \n",
    "        # Compute metrics\n",
    "        metrics = {\n",
    "            'loss': loss,\n",
    "            'accuracy': jnp.mean((jax.nn.sigmoid(logits) > 0.5) == batch['labels'])\n",
    "        }\n",
    "        \n",
    "        return state, metrics\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate on validation set.\"\"\"\n",
    "        metrics_list = []\n",
    "        \n",
    "        for i in range(0, len(self.valid_data['timeseries']), self.config.batch_size):\n",
    "            batch_idx = slice(i, i + self.config.batch_size)\n",
    "            batch = {\n",
    "                'timeseries': self.valid_data['timeseries'][batch_idx],\n",
    "                'images': self.valid_data['images'][batch_idx],\n",
    "                'labels': self.valid_data['labels'][batch_idx]\n",
    "            }\n",
    "            \n",
    "            logits = self.state.apply_fn(\n",
    "                {'params': self.state.params},\n",
    "                (batch['timeseries'], batch['images'])\n",
    "            )\n",
    "            \n",
    "            loss = optax.sigmoid_binary_cross_entropy(logits, batch['labels']).mean()\n",
    "            accuracy = jnp.mean((jax.nn.sigmoid(logits) > 0.5) == batch['labels'])\n",
    "            \n",
    "            metrics_list.append({'loss': loss, 'accuracy': accuracy})\n",
    "        \n",
    "        # Average metrics\n",
    "        return {\n",
    "            k: float(np.mean([m[k] for m in metrics_list]))\n",
    "            for k in metrics_list[0].keys()\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    try:\n",
    "        # Initialize config and data processor\n",
    "        config = Config()\n",
    "        processor = DataProcessor(config)\n",
    "        \n",
    "        print(\"Preparing datasets...\")\n",
    "        train_data, valid_data, test_data = processor.prepare_datasets()\n",
    "        \n",
    "        print(\"\\nDataset shapes:\")\n",
    "        print(f\"Train: {train_data['timeseries'].shape}\")\n",
    "        print(f\"Valid: {valid_data['timeseries'].shape}\")\n",
    "        print(f\"Test: {test_data['timeseries'].shape}\")\n",
    "        \n",
    "        # Initialize model and trainer\n",
    "        print(\"\\nInitializing model...\")\n",
    "        model = FloodDetectionModel(config=config)\n",
    "        \n",
    "        print(\"Setting up trainer...\")\n",
    "        trainer = Trainer(config, model, train_data, valid_data)\n",
    "        \n",
    "        # Train model\n",
    "        print(\"\\nStarting training...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save model\n",
    "        print(\"\\nSaving model...\")\n",
    "        with open(config.base_path / 'flood_detection_model.pkl', 'wb') as f:\n",
    "            pickle.dump(trainer.state, f)\n",
    "        \n",
    "        print(\"Training complete!\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying data files...\n",
      "Loading data...\n",
      "Training data columns: Index(['event_id', 'precipitation', 'label'], dtype='object')\n",
      "Test data columns: Index(['event_id', 'precipitation'], dtype='object')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"'event_t' column not found in training data\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 193\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 193\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[17], line 106\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    103\u001b[0m verify_data_files()\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 106\u001b[0m data, data_test \u001b[38;5;241m=\u001b[39m load_and_preprocess_data()\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Load image data\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading image data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 91\u001b[0m, in \u001b[0;36mload_and_preprocess_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Check if 'event_t' column exists\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_t\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_t\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m column not found in training data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_t\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data_test\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_t\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m column not found in test data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"'event_t' column not found in training data\""
     ]
    }
   ],
   "source": [
    "class TrainingVisualizer:\n",
    "    \"\"\"Visualize training progress and results.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.metrics_history = {\n",
    "            'train_loss': [], 'train_acc': [],\n",
    "            'valid_loss': [], 'valid_acc': [],\n",
    "            'valid_auroc': []\n",
    "        }\n",
    "        \n",
    "    def update_metrics(self, train_metrics, valid_metrics):\n",
    "        \"\"\"Update metrics history.\"\"\"\n",
    "        self.metrics_history['train_loss'].append(np.mean([m['loss'] for m in train_metrics]))\n",
    "        self.metrics_history['train_acc'].append(np.mean([m['accuracy'] for m in train_metrics]))\n",
    "        self.metrics_history['valid_loss'].append(np.mean([m['focal_loss'] for m in valid_metrics]))\n",
    "        self.metrics_history['valid_acc'].append(np.mean([m['accuracy'] for m in valid_metrics]))\n",
    "        self.metrics_history['valid_auroc'].append(np.mean([m['auroc'] for m in valid_metrics]))\n",
    "    \n",
    "    def plot_metrics(self):\n",
    "        \"\"\"Plot training and validation metrics.\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot losses\n",
    "        ax1.plot(self.metrics_history['train_loss'], label='Train Loss')\n",
    "        ax1.plot(self.metrics_history['valid_loss'], label='Valid Loss')\n",
    "        ax1.set_title('Loss Over Time')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Plot accuracies\n",
    "        ax2.plot(self.metrics_history['train_acc'], label='Train Acc')\n",
    "        ax2.plot(self.metrics_history['valid_acc'], label='Valid Acc')\n",
    "        ax2.plot(self.metrics_history['valid_auroc'], label='Valid AUROC')\n",
    "        ax2.set_title('Metrics Over Time')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Score')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_prediction_examples(self, dataset, model, num_examples=5):\n",
    "        \"\"\"Plot example predictions.\"\"\"\n",
    "        rng = np.random.default_rng(42)\n",
    "        indices = rng.choice(len(dataset), num_examples)\n",
    "        \n",
    "        fig, axes = plt.subplots(num_examples, 2, figsize=(12, 4*num_examples))\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            # Get sample\n",
    "            (ts, img), label = dataset.get_batch([idx])\n",
    "            \n",
    "            # Make prediction\n",
    "            logits = model.apply({'params': model.params}, (ts, img))\n",
    "            pred = jax.nn.sigmoid(logits)\n",
    "            \n",
    "            # Plot time series\n",
    "            axes[i, 0].plot(ts[0, :, 0], label='Precipitation')\n",
    "            if label is not None:\n",
    "                axes[i, 0].plot(label[0], label='True Flood', alpha=0.5)\n",
    "            axes[i, 0].plot(pred[0], label='Predicted Prob', alpha=0.5)\n",
    "            axes[i, 0].set_title(f'Time Series - Sample {i+1}')\n",
    "            axes[i, 0].legend()\n",
    "            \n",
    "            # Plot satellite image (RGB composite)\n",
    "            rgb_img = img[0, :, :, [2,1,0]]  # Use bands 4,3,2 for RGB\n",
    "            rgb_img = (rgb_img - rgb_img.min()) / (rgb_img.max() - rgb_img.min())\n",
    "            axes[i, 1].imshow(rgb_img)\n",
    "            axes[i, 1].set_title(f'Satellite Image - Sample {i+1}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Define the base path\n",
    "BASE_PATH = r'C:\\Users\\CraigParker\\OneDrive - Wits Health Consortium\\PHR PC\\Downloads\\Joburg\\Zindi_comp'\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"Load and preprocess the data.\"\"\"\n",
    "    try:\n",
    "        # Load CSV files\n",
    "        data = pd.read_csv(os.path.join(BASE_PATH, 'Train.csv'))\n",
    "        data_test = pd.read_csv(os.path.join(BASE_PATH, 'Test.csv'))\n",
    "        \n",
    "        # Debug: Print initial data info\n",
    "        print(\"\\nInitial data shapes:\")\n",
    "        print(f\"Training data: {data.shape}\")\n",
    "        print(f\"Test data: {data_test.shape}\")\n",
    "        print(\"\\nTraining columns:\", data.columns.tolist())\n",
    "        \n",
    "        # Process event IDs\n",
    "        for df in [data, data_test]:\n",
    "            # Clean event IDs\n",
    "            df['event_id'] = df['event_id'].apply(lambda x: '_'.join(x.split('_')[:2]))\n",
    "            # Create event_t column (time steps within each event)\n",
    "            df['event_t'] = df.groupby('event_id').cumcount()\n",
    "        \n",
    "        # Verify created columns\n",
    "        print(\"\\nAfter preprocessing:\")\n",
    "        print(\"Training data columns:\", data.columns.tolist())\n",
    "        print(\"Sample event_t values:\", data['event_t'].head())\n",
    "        print(\"\\nUnique events:\", len(data['event_id'].unique()))\n",
    "        print(\"Max time steps:\", data['event_t'].max())\n",
    "        \n",
    "        # Verify event_t creation\n",
    "        if 'event_t' not in data.columns:\n",
    "            raise KeyError(\"Failed to create 'event_t' column in training data\")\n",
    "        if 'event_t' not in data_test.columns:\n",
    "            raise KeyError(\"Failed to create 'event_t' column in test data\")\n",
    "        \n",
    "        # Verify data integrity\n",
    "        for df, name in [(data, 'training'), (data_test, 'test')]:\n",
    "            # Check for required columns\n",
    "            required_cols = ['event_id', 'event_t', 'precipitation']\n",
    "            missing_cols = set(required_cols) - set(df.columns)\n",
    "            if missing_cols:\n",
    "                raise KeyError(f\"Missing columns in {name} data: {missing_cols}\")\n",
    "            \n",
    "            # Check for null values\n",
    "            null_counts = df[required_cols].isnull().sum()\n",
    "            if null_counts.any():\n",
    "                print(f\"\\nWarning: Found null values in {name} data:\")\n",
    "                print(null_counts[null_counts > 0])\n",
    "        \n",
    "        return data, data_test\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in data preprocessing:\")\n",
    "        print(f\"Type: {type(e).__name__}\")\n",
    "        print(f\"Message: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def validate_dataset(data: pd.DataFrame, name: str = \"dataset\"):\n",
    "    \"\"\"Validate dataset integrity and format.\"\"\"\n",
    "    print(f\"\\nValidating {name}...\")\n",
    "    \n",
    "    # Check basic properties\n",
    "    print(f\"Shape: {data.shape}\")\n",
    "    print(f\"Columns: {data.columns.tolist()}\")\n",
    "    \n",
    "    # Check event structure\n",
    "    num_events = len(data['event_id'].unique())\n",
    "    timesteps_per_event = data.groupby('event_id').size()\n",
    "    print(f\"\\nNumber of unique events: {num_events}\")\n",
    "    print(f\"Timesteps per event:\")\n",
    "    print(f\"  Min: {timesteps_per_event.min()}\")\n",
    "    print(f\"  Max: {timesteps_per_event.max()}\")\n",
    "    print(f\"  Mean: {timesteps_per_event.mean():.2f}\")\n",
    "    \n",
    "    # Check data types\n",
    "    print(\"\\nData types:\")\n",
    "    print(data.dtypes)\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing = data.isnull().sum()\n",
    "    if missing.any():\n",
    "        print(\"\\nMissing values:\")\n",
    "        print(missing[missing > 0])\n",
    "    \n",
    "    # Check precipitation values\n",
    "    if 'precipitation' in data.columns:\n",
    "        precip = data['precipitation']\n",
    "        print(\"\\nPrecipitation statistics:\")\n",
    "        print(f\"  Min: {precip.min():.2f}\")\n",
    "        print(f\"  Max: {precip.max():.2f}\")\n",
    "        print(f\"  Mean: {precip.mean():.2f}\")\n",
    "        print(f\"  Std: {precip.std():.2f}\")\n",
    "    \n",
    "    # Check label distribution if present\n",
    "    if 'label' in data.columns:\n",
    "        label_dist = data['label'].value_counts(normalize=True)\n",
    "        print(\"\\nLabel distribution:\")\n",
    "        print(label_dist)\n",
    "    \n",
    "    print(\"\\nValidation complete.\")\n",
    "    return True\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    print(\"Verifying data files...\")\n",
    "    verify_data_files()\n",
    "    \n",
    "    print(\"Loading data...\")\n",
    "    data, data_test = load_and_preprocess_data()\n",
    "    \n",
    "    # Load image data\n",
    "    print(\"Loading image data...\")\n",
    "    images_path = os.path.join(BASE_PATH, 'composite_images.npz')\n",
    "    images = np.load(images_path)\n",
    "    \n",
    "    # Create train/validation split\n",
    "    print(\"Creating dataset splits...\")\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    event_ids = data['event_id'].unique()\n",
    "    validation_size = int(len(event_ids) * 0.1)\n",
    "    valid_ids = rng.choice(event_ids, size=validation_size, replace=False)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_mask = ~data['event_id'].isin(valid_ids)\n",
    "    valid_mask = data['event_id'].isin(valid_ids)\n",
    "    \n",
    "    # Initialize datasets\n",
    "    train_dataset = FloodDataset(\n",
    "        timeseries=data[train_mask].pivot(index='event_id', columns='event_t', values='precipitation').to_numpy(),\n",
    "        images=np.stack([images[id] for id in data[train_mask]['event_id'].unique()]),\n",
    "        labels=data[train_mask].pivot(index='event_id', columns='event_t', values='label').to_numpy(),\n",
    "        is_training=True\n",
    "    )\n",
    "    \n",
    "    valid_dataset = FloodDataset(\n",
    "        timeseries=data[valid_mask].pivot(index='event_id', columns='event_t', values='precipitation').to_numpy(),\n",
    "        images=np.stack([images[id] for id in data[valid_mask]['event_id'].unique()]),\n",
    "        labels=data[valid_mask].pivot(index='event_id', columns='event_t', values='label').to_numpy(),\n",
    "        is_training=False\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"Initializing model...\")\n",
    "    model_config = {\n",
    "        'patch_size': 16,\n",
    "        'hidden_dim': 512,\n",
    "        'num_layers': 8,\n",
    "        'num_heads': 8,\n",
    "        'dropout': 0.1,\n",
    "        'input_shape': (32, 128, 128, 6)  # Add input shape\n",
    "    }\n",
    "    \n",
    "    model = SatelliteViT(**model_config)\n",
    "    \n",
    "    # Initialize trainer\n",
    "    print(\"Setting up trainer...\")\n",
    "    optimizer_config = {\n",
    "        'learning_rate': 1e-4,\n",
    "        'weight_decay': 0.01\n",
    "    }\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        optimizer_config=optimizer_config,\n",
    "        train_dataset=train_dataset,\n",
    "        valid_dataset=valid_dataset,\n",
    "        num_epochs=100,\n",
    "        batch_size=32,\n",
    "        steps_per_eval=5\n",
    "    )\n",
    "    \n",
    "    # Initialize visualizer\n",
    "    visualizer = TrainingVisualizer()\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"Starting training...\")\n",
    "    try:\n",
    "        trainer.train(visualizer)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user\")\n",
    "    \n",
    "    # Plot final results\n",
    "    print(\"Plotting results...\")\n",
    "    visualizer.plot_metrics()\n",
    "    visualizer.plot_prediction_examples(valid_dataset, model)\n",
    "    \n",
    "    # Save model\n",
    "    print(\"Saving model...\")\n",
    "    model_path = os.path.join(BASE_PATH, 'flood_detection_model.pkl')\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(trainer.state, f)\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot concatenate arrays with different numbers of dimensions: got (1, 128), (1, 10, 128).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 370\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;66;03m# We won't implement predictions on test_data here,\u001b[39;00m\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;66;03m# but you could follow the same approach used in `evaluate()`.\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 370\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[23], line 350\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;66;03m# Initialize model and trainer\u001b[39;00m\n\u001b[0;32m    349\u001b[0m model \u001b[38;5;241m=\u001b[39m SimpleFloodModel(config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[1;32m--> 350\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(config, model, train_data, valid_data)\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m    353\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtraining_loop()\n",
      "Cell \u001b[1;32mIn[23], line 243\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, config, model, train_data, valid_data)\u001b[0m\n\u001b[0;32m    241\u001b[0m dummy_ts \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mjnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    242\u001b[0m dummy_img \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mjnp\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# Dummy shape\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m variables \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minit(rng, dummy_ts, dummy_img)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;66;03m# Create optimizer schedule (e.g., simple constant or AdamW)\u001b[39;00m\n\u001b[0;32m    246\u001b[0m tx \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39madamw(learning_rate\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mLEARNING_RATE, weight_decay\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mWEIGHT_DECAY)\n",
      "    \u001b[1;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[23], line 220\u001b[0m, in \u001b[0;36mSimpleFloodModel.__call__\u001b[1;34m(self, timeseries, images, train)\u001b[0m\n\u001b[0;32m    217\u001b[0m x_img_tiled \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mtile(x_img, (\u001b[38;5;241m1\u001b[39m, T, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# 3) Combine time-series + image encodings\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m combined \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mconcatenate([x_ts, x_img_tiled], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# Project down to (batch_size, T, 1)\u001b[39;00m\n\u001b[0;32m    223\u001b[0m logits \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1\u001b[39m)(combined)\n",
      "File \u001b[1;32mc:\\Users\\CraigParker\\anaconda3\\Lib\\site-packages\\jax\\_src\\numpy\\lax_numpy.py:4788\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(arrays, axis, dtype)\u001b[0m\n\u001b[0;32m   4786\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[0;32m   4787\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(arrays_out) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m-> 4788\u001b[0m   arrays_out \u001b[38;5;241m=\u001b[39m [lax\u001b[38;5;241m.\u001b[39mconcatenate(arrays_out[i:i\u001b[38;5;241m+\u001b[39mk], axis)\n\u001b[0;32m   4789\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(arrays_out), k)]\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_out[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\CraigParker\\anaconda3\\Lib\\site-packages\\jax\\_src\\numpy\\lax_numpy.py:4788\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   4786\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[0;32m   4787\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(arrays_out) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m-> 4788\u001b[0m   arrays_out \u001b[38;5;241m=\u001b[39m [lax\u001b[38;5;241m.\u001b[39mconcatenate(arrays_out[i:i\u001b[38;5;241m+\u001b[39mk], axis)\n\u001b[0;32m   4789\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(arrays_out), k)]\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_out[\u001b[38;5;241m0\u001b[39m]\n",
      "    \u001b[1;31m[... skipping hidden 25 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\CraigParker\\anaconda3\\Lib\\site-packages\\jax\\_src\\lax\\lax.py:4442\u001b[0m, in \u001b[0;36m_concatenate_shape_rule\u001b[1;34m(*operands, **kwargs)\u001b[0m\n\u001b[0;32m   4440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m({operand\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;28;01mfor\u001b[39;00m operand \u001b[38;5;129;01min\u001b[39;00m operands}) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4441\u001b[0m   msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot concatenate arrays with different numbers of dimensions: got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 4442\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(o\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m operands)))\n\u001b[0;32m   4443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m dimension \u001b[38;5;241m<\u001b[39m operands[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mndim:\n\u001b[0;32m   4444\u001b[0m   msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcatenate dimension out of bounds: dimension \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m for shapes \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot concatenate arrays with different numbers of dimensions: got (1, 128), (1, 10, 128)."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simplified Flood Detection Model for South Africa\n",
    "\n",
    "This script:\n",
    "1) Verifies that all required data files exist.\n",
    "2) Loads and preprocesses the training and test data.\n",
    "3) Builds a simplified Transformer-based model with Flax/JAX.\n",
    "4) Trains the model on the training set and evaluates on a validation set.\n",
    "5) Saves the final model state.\n",
    "\n",
    "Dependencies (install if needed):\n",
    "  pip install jax jaxlib flax optax\n",
    "  pip install numpy pandas matplotlib tqdm\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. Configuration\n",
    "# -----------------------------------------------------------------------------\n",
    "class Config:\n",
    "    \"\"\"Holds paths and hyperparameters for the project.\"\"\"\n",
    "    def __init__(self):\n",
    "        # Paths\n",
    "        self.BASE_PATH = Path(\n",
    "            r\"C:\\Users\\CraigParker\\OneDrive - Wits Health Consortium\\PHR PC\\Downloads\\Joburg\\Zindi_comp\"\n",
    "        )\n",
    "        self.REQUIRED_FILES = [\"Train.csv\", \"Test.csv\", \"composite_images.npz\", \"SampleSubmission.csv\"]\n",
    "\n",
    "        # Random seed\n",
    "        self.SEED = 42\n",
    "\n",
    "        # Dataset\n",
    "        self.VALID_SPLIT = 0.1  # 10% validation split\n",
    "        self.BATCH_SIZE = 32\n",
    "\n",
    "        # Model\n",
    "        self.TIME_SERIES_HIDDEN_DIM = 128\n",
    "        self.IMG_HIDDEN_DIM = 128\n",
    "        self.NUM_LAYERS = 4\n",
    "        self.NUM_HEADS = 4\n",
    "        self.DROPOUT = 0.1\n",
    "\n",
    "        # Training\n",
    "        self.NUM_EPOCHS = 5\n",
    "        self.LEARNING_RATE = 1e-4\n",
    "        self.WEIGHT_DECAY = 0.01\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. Verify Required Data Files\n",
    "# -----------------------------------------------------------------------------\n",
    "def verify_data_files(config: Config) -> None:\n",
    "    \"\"\"Check if all required data files exist in the specified path.\"\"\"\n",
    "    missing_files = []\n",
    "    for file_name in config.REQUIRED_FILES:\n",
    "        file_path = config.BASE_PATH / file_name\n",
    "        if not file_path.exists():\n",
    "            missing_files.append(file_name)\n",
    "\n",
    "    if missing_files:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Missing files in {config.BASE_PATH}: {', '.join(missing_files)}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. Data Loading and Preprocessing\n",
    "# -----------------------------------------------------------------------------\n",
    "class DataProcessor:\n",
    "    \"\"\"Loads and preprocesses CSV files and image data.\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.rng = np.random.default_rng(config.SEED)\n",
    "\n",
    "    def load_raw_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, np.ndarray]]:\n",
    "        \"\"\"Loads the CSV files and the composite images dictionary.\"\"\"\n",
    "        # Load train/test CSV\n",
    "        train_df = pd.read_csv(self.config.BASE_PATH / \"Train.csv\")\n",
    "        test_df = pd.read_csv(self.config.BASE_PATH / \"Test.csv\")\n",
    "\n",
    "        # Clean event_id (remove trailing \"_XXX\")\n",
    "        for df in [train_df, test_df]:\n",
    "            df[\"event_id\"] = df[\"event_id\"].apply(lambda x: \"_\".join(x.split(\"_\")[:2]))\n",
    "            # Create a time-step column for each event\n",
    "            df[\"event_t\"] = df.groupby(\"event_id\").cumcount()\n",
    "\n",
    "        # Load images (npz file = dictionary of event_id -> 3D array)\n",
    "        images_dict = dict(np.load(self.config.BASE_PATH / \"composite_images.npz\", allow_pickle=True))\n",
    "\n",
    "        return train_df, test_df, images_dict\n",
    "\n",
    "    def prepare_datasets(self) -> Tuple[Dict, Dict, Dict]:\n",
    "        \"\"\"Split the training data into train/val sets, and set up test data.\"\"\"\n",
    "        train_df, test_df, images_dict = self.load_raw_data()\n",
    "\n",
    "        # Split event IDs for train/validation\n",
    "        train_events, valid_events = train_test_split(\n",
    "            train_df[\"event_id\"].unique(),\n",
    "            test_size=self.config.VALID_SPLIT,\n",
    "            random_state=self.config.SEED\n",
    "        )\n",
    "\n",
    "        # A helper to pivot precipitation into timeseries shape\n",
    "        def pivot_timeseries(df: pd.DataFrame, events: np.ndarray) -> np.ndarray:\n",
    "            subset = df[df[\"event_id\"].isin(events)]\n",
    "            ts_matrix = subset.pivot(\n",
    "                index=\"event_id\", columns=\"event_t\", values=\"precipitation\"\n",
    "            ).fillna(0).to_numpy()\n",
    "            return ts_matrix\n",
    "\n",
    "        # A helper to get stacked images for each event\n",
    "        def gather_images(events: np.ndarray) -> np.ndarray:\n",
    "            return np.stack([images_dict[e] for e in events])\n",
    "\n",
    "        # A helper to pivot labels if they exist\n",
    "        def pivot_labels(df: pd.DataFrame, events: np.ndarray) -> np.ndarray:\n",
    "            if \"label\" not in df.columns:\n",
    "                return None\n",
    "            subset = df[df[\"event_id\"].isin(events)]\n",
    "            label_matrix = subset.pivot(\n",
    "                index=\"event_id\", columns=\"event_t\", values=\"label\"\n",
    "            ).fillna(0).to_numpy()\n",
    "            return label_matrix\n",
    "\n",
    "        # Build dictionary-based data splits\n",
    "        train_data = {\n",
    "            \"timeseries\": pivot_timeseries(train_df, train_events),\n",
    "            \"images\": gather_images(train_events),\n",
    "            \"labels\": pivot_labels(train_df, train_events),\n",
    "        }\n",
    "        valid_data = {\n",
    "            \"timeseries\": pivot_timeseries(train_df, valid_events),\n",
    "            \"images\": gather_images(valid_events),\n",
    "            \"labels\": pivot_labels(train_df, valid_events),\n",
    "        }\n",
    "        test_events = test_df[\"event_id\"].unique()\n",
    "        test_data = {\n",
    "            \"timeseries\": pivot_timeseries(test_df, test_events),\n",
    "            \"images\": gather_images(test_events),\n",
    "            \"labels\": pivot_labels(test_df, test_events),  # Usually None\n",
    "        }\n",
    "\n",
    "        return train_data, valid_data, test_data\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. Model Definition\n",
    "# -----------------------------------------------------------------------------\n",
    "class SimpleFloodModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified model that:\n",
    "      - Encodes time-series data with a small Transformer.\n",
    "      - Encodes images by flattening them and passing through a small Transformer.\n",
    "      - Merges both features and outputs a per-time-step prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    config: Config\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, timeseries: jnp.ndarray, images: jnp.ndarray, train: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          timeseries: (batch_size, T, ?) numeric data (precip, etc.)\n",
    "          images:     (batch_size, H, W, C) satellite images\n",
    "        Returns:\n",
    "          (batch_size, T) float predictions\n",
    "        \"\"\"\n",
    "        # 1) Time-series encoder\n",
    "        # Project the timeseries data\n",
    "        x_ts = nn.Dense(self.config.TIME_SERIES_HIDDEN_DIM)(timeseries.reshape(timeseries.shape[0], -1))\n",
    "\n",
    "        # Apply a few transformer layers on time-series\n",
    "        for _ in range(self.config.NUM_LAYERS):\n",
    "            # Normalization\n",
    "            y = nn.LayerNorm()(x_ts)\n",
    "            # Self-attention\n",
    "            y = nn.MultiHeadDotProductAttention(num_heads=self.config.NUM_HEADS)(\n",
    "                y, y, deterministic=not train\n",
    "            )\n",
    "            x_ts = x_ts + y  # residual\n",
    "\n",
    "        # 2) Image encoder (extremely simplified)\n",
    "        # Flatten images: shape (B, H*W*C)\n",
    "        b, h, w, c = images.shape\n",
    "        x_img = images.reshape(b, h * w * c)\n",
    "        x_img = nn.Dense(self.config.IMG_HIDDEN_DIM)(x_img)\n",
    "\n",
    "        # Just do one transformer layer on the flattened image\n",
    "        x_img = jnp.expand_dims(x_img, axis=1)  # shape (B, 1, hidden_dim)\n",
    "        for _ in range(1):  # let's do 1 layer only\n",
    "            y = nn.LayerNorm()(x_img)\n",
    "            y = nn.MultiHeadDotProductAttention(num_heads=1)(y, y, deterministic=not train)\n",
    "            x_img = x_img + y\n",
    "\n",
    "        # Expand so we can concatenate with the time-series dimension\n",
    "        # We'll replicate the single image feature across T steps\n",
    "        T = timeseries.shape[1]\n",
    "        x_img_tiled = jnp.tile(x_img, (1, T, 1))\n",
    "\n",
    "        # 3) Combine time-series + image encodings\n",
    "        combined = jnp.concatenate([x_ts, x_img_tiled], axis=-1)\n",
    "\n",
    "        # Project down to (batch_size, T, 1)\n",
    "        logits = nn.Dense(1)(combined)\n",
    "        return jnp.squeeze(logits, axis=-1)  # shape (B, T)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. Trainer Definition\n",
    "# -----------------------------------------------------------------------------\n",
    "class Trainer:\n",
    "    def __init__(self, config: Config, model: nn.Module, train_data: Dict, valid_data: Dict):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.train_data = train_data\n",
    "        self.valid_data = valid_data\n",
    "\n",
    "        # Create a training state (params, optimizer)\n",
    "        rng = jax.random.PRNGKey(config.SEED)\n",
    "\n",
    "        # Dummy batch to initialize model\n",
    "        dummy_ts = jnp.ones((1, 10, 1), dtype=jnp.float32)\n",
    "        dummy_img = jnp.ones((1, 8, 8, 6), dtype=jnp.float32)  # Dummy shape\n",
    "        variables = model.init(rng, dummy_ts, dummy_img)\n",
    "\n",
    "        # Create optimizer schedule (e.g., simple constant or AdamW)\n",
    "        tx = optax.adamw(learning_rate=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY)\n",
    "\n",
    "        self.state = train_state.TrainState.create(\n",
    "            apply_fn=model.apply, params=variables[\"params\"], tx=tx\n",
    "        )\n",
    "\n",
    "    def training_loop(self):\n",
    "        \"\"\"Run a simple training loop.\"\"\"\n",
    "        # Convert data to jax arrays once\n",
    "        train_ts = jnp.array(self.train_data[\"timeseries\"], dtype=jnp.float32)\n",
    "        train_img = jnp.array(self.train_data[\"images\"], dtype=jnp.float32)\n",
    "        train_label = (\n",
    "            jnp.array(self.train_data[\"labels\"], dtype=jnp.float32)\n",
    "            if self.train_data[\"labels\"] is not None\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        num_samples = train_ts.shape[0]\n",
    "        num_steps = (num_samples // self.config.BATCH_SIZE) or 1\n",
    "\n",
    "        for epoch in range(self.config.NUM_EPOCHS):\n",
    "            # Shuffle data\n",
    "            perm = self._rng_permutation(num_samples)\n",
    "            train_ts_shuffled = train_ts[perm]\n",
    "            train_img_shuffled = train_img[perm]\n",
    "            train_label_shuffled = train_label[perm] if train_label is not None else None\n",
    "\n",
    "            epoch_losses = []\n",
    "\n",
    "            with tqdm(range(num_steps), desc=f\"Epoch {epoch+1}/{self.config.NUM_EPOCHS}\") as pbar:\n",
    "                for step_idx in pbar:\n",
    "                    # Mini-batch slice\n",
    "                    start = step_idx * self.config.BATCH_SIZE\n",
    "                    end = start + self.config.BATCH_SIZE\n",
    "\n",
    "                    batch_ts = train_ts_shuffled[start:end]\n",
    "                    batch_img = train_img_shuffled[start:end]\n",
    "                    batch_label = train_label_shuffled[start:end] if train_label_shuffled is not None else None\n",
    "\n",
    "                    # Training step\n",
    "                    self.state, loss_val = self._train_step(self.state, batch_ts, batch_img, batch_label)\n",
    "                    epoch_losses.append(float(loss_val))\n",
    "                    pbar.set_postfix({\"loss\": f\"{np.mean(epoch_losses):.4f}\"})\n",
    "\n",
    "            # After each epoch, do a quick validation if labels exist\n",
    "            if self.valid_data[\"labels\"] is not None:\n",
    "                valid_metrics = self.evaluate(self.valid_data)\n",
    "                print(f\"Validation after epoch {epoch+1}: loss={valid_metrics['loss']:.4f}\")\n",
    "\n",
    "        print(\"Training completed.\")\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def _train_step(self, state, timeseries, images, labels):\n",
    "        \"\"\"Single training step.\"\"\"\n",
    "        def loss_fn(params):\n",
    "            logits = self.model.apply({\"params\": params}, timeseries, images, train=True)\n",
    "            if labels is None:\n",
    "                # If no labels, no supervised loss\n",
    "                return 0.0, logits\n",
    "            # Simple binary cross-entropy\n",
    "            loss = optax.sigmoid_binary_cross_entropy(logits, labels).mean()\n",
    "            return loss, logits\n",
    "\n",
    "        (loss, _), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "        new_state = state.apply_gradients(grads=grads)\n",
    "        return new_state, loss\n",
    "\n",
    "    def evaluate(self, data_split: Dict):\n",
    "        \"\"\"Compute average loss on a given dataset split.\"\"\"\n",
    "        timeseries = jnp.array(data_split[\"timeseries\"], dtype=jnp.float32)\n",
    "        images = jnp.array(data_split[\"images\"], dtype=jnp.float32)\n",
    "        labels = data_split[\"labels\"]\n",
    "        if labels is None:\n",
    "            return {\"loss\": np.nan}\n",
    "        labels = jnp.array(labels, dtype=jnp.float32)\n",
    "\n",
    "        logits = self.model.apply({\"params\": self.state.params}, timeseries, images, train=False)\n",
    "        loss = optax.sigmoid_binary_cross_entropy(logits, labels).mean()\n",
    "        return {\"loss\": float(loss)}\n",
    "\n",
    "    def _rng_permutation(self, n):\n",
    "        \"\"\"Return a random permutation of indices [0..n-1], for data shuffling.\"\"\"\n",
    "        # We can do CPU-based permutation in NumPy, then convert\n",
    "        indices = np.arange(n)\n",
    "        self.config.SEED += 1  # increment seed each time for variety\n",
    "        np.random.default_rng(self.config.SEED).shuffle(indices)\n",
    "        return jnp.array(indices)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6. Main Script\n",
    "# -----------------------------------------------------------------------------\n",
    "def main():\n",
    "    config = Config()\n",
    "\n",
    "    # Verify required files\n",
    "    verify_data_files(config)\n",
    "\n",
    "    # Load and prepare datasets\n",
    "    processor = DataProcessor(config)\n",
    "    train_data, valid_data, test_data = processor.prepare_datasets()\n",
    "\n",
    "    # Initialize model and trainer\n",
    "    model = SimpleFloodModel(config=config)\n",
    "    trainer = Trainer(config, model, train_data, valid_data)\n",
    "\n",
    "    # Train model\n",
    "    trainer.training_loop()\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    if valid_data[\"labels\"] is not None:\n",
    "        val_metrics = trainer.evaluate(valid_data)\n",
    "        print(f\"Final validation loss: {val_metrics['loss']:.4f}\")\n",
    "\n",
    "    # Save model parameters\n",
    "    with open(config.BASE_PATH / \"flood_detection_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(trainer.state, f)\n",
    "    print(\"Model saved successfully.\")\n",
    "\n",
    "    # We won't implement predictions on test_data here,\n",
    "    # but you could follow the same approach used in `evaluate()`.\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
