{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Data files not found in Zindi_comp",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 136\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# Add training steps...\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 136\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[1], line 126\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Main execution function.\"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 126\u001b[0m data, data_test \u001b[38;5;241m=\u001b[39m load_and_preprocess_data()\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing time series...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    129\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m TimeSeriesPreprocessor()\n",
      "Cell \u001b[1;32mIn[1], line 104\u001b[0m, in \u001b[0;36mload_and_preprocess_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m    101\u001b[0m test_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(train_path) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(test_path):\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData files not found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    106\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(train_path)\n\u001b[0;32m    107\u001b[0m data_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(test_path)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Data files not found in Zindi_comp"
     ]
    }
   ],
   "source": [
    "# Improved Flood Detection Model for South Africa\n",
    "# Based on DeepMind's starter notebook with enhancements\n",
    "\n",
    "import os\n",
    "import math\n",
    "import collections\n",
    "from collections.abc import Callable, Sequence\n",
    "from functools import partial\n",
    "from typing import Any\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# Deep Learning\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(seed=SEED)\n",
    "\n",
    "# Constants\n",
    "BASE_PATH = 'Zindi_comp'  # Update this path to your data directory\n",
    "NUM_CLASSES = 2\n",
    "BAND_NAMES = ('B2', 'B3', 'B4', 'B8', 'B11', 'slope')\n",
    "H, W, NUM_CHANNELS = IMG_DIM = (128, 128, len(BAND_NAMES))\n",
    "\n",
    "class TimeSeriesPreprocessor:\n",
    "    \"\"\"Enhanced time series preprocessing with advanced feature engineering.\"\"\"\n",
    "    def __init__(self, window_sizes=[3, 7, 14, 30]):\n",
    "        self.window_sizes = window_sizes\n",
    "    \n",
    "    def add_rolling_features(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Add rolling window statistics for precipitation.\"\"\"\n",
    "        features = [data]\n",
    "        \n",
    "        for window in self.window_sizes:\n",
    "            # Rolling mean\n",
    "            rolling_mean = np.apply_along_axis(\n",
    "                lambda x: np.convolve(x, np.ones(window)/window, mode='same'),\n",
    "                axis=1, arr=data)\n",
    "            features.append(rolling_mean)\n",
    "            \n",
    "            # Rolling max\n",
    "            rolling_max = np.apply_along_axis(\n",
    "                lambda x: np.maximum.accumulate(x, axis=0),\n",
    "                axis=1, arr=rolling_mean)\n",
    "            features.append(rolling_max)\n",
    "            \n",
    "            # Rolling std\n",
    "            rolling_std = np.apply_along_axis(\n",
    "                lambda x: np.std(x[max(0, len(x)-window):]),\n",
    "                axis=1, arr=data)\n",
    "            features.append(np.expand_dims(rolling_std, -1))\n",
    "        \n",
    "        # Add cumulative precipitation\n",
    "        cumsum = np.cumsum(data, axis=1)\n",
    "        features.append(cumsum)\n",
    "        \n",
    "        # Add smoothed signal\n",
    "        smoothed = gaussian_filter1d(data, sigma=2.0, axis=1)\n",
    "        features.append(smoothed)\n",
    "        \n",
    "        return np.concatenate(features, axis=-1)\n",
    "    \n",
    "    def transform(self, timeseries: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Transform time series data with all features.\"\"\"\n",
    "        enhanced_features = self.add_rolling_features(timeseries)\n",
    "        \n",
    "        # Normalize features\n",
    "        mean = np.mean(enhanced_features, axis=(0, 1), keepdims=True)\n",
    "        std = np.std(enhanced_features, axis=(0, 1), keepdims=True)\n",
    "        normalized = (enhanced_features - mean) / (std + 1e-8)\n",
    "        \n",
    "        return normalized.astype(np.float32)\n",
    "\n",
    "def sigmoid_focal_loss(logits, labels, alpha=0.25, gamma=2.0):\n",
    "    \"\"\"Compute focal loss for binary classification.\"\"\"\n",
    "    p = jnp.clip(jax.nn.sigmoid(logits), 1e-6, 1-1e-6)\n",
    "    ce_loss = -(labels * jnp.log(p) + (1 - labels) * jnp.log(1 - p))\n",
    "    p_t = labels * p + (1 - labels) * (1 - p)\n",
    "    focal_weight = jnp.power(1 - p_t, gamma)\n",
    "    alpha_weight = labels * alpha + (1 - labels) * (1 - alpha)\n",
    "    return focal_weight * alpha_weight * ce_loss\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"Load and preprocess all data.\"\"\"\n",
    "    # Load CSV files\n",
    "    train_path = os.path.join(BASE_PATH, 'Train.csv')\n",
    "    test_path = os.path.join(BASE_PATH, 'Test.csv')\n",
    "    \n",
    "    if not os.path.exists(train_path) or not os.path.exists(test_path):\n",
    "        raise FileNotFoundError(f\"Data files not found in {BASE_PATH}\")\n",
    "    \n",
    "    data = pd.read_csv(train_path)\n",
    "    data_test = pd.read_csv(test_path)\n",
    "    \n",
    "    # Process event IDs\n",
    "    data['event_id'] = data['event_id'].apply(lambda x: '_'.join(x.split('_')[0:2]))\n",
    "    data['event_idx'] = data.groupby('event_id', sort=False).ngroup()\n",
    "    data_test['event_id'] = data_test['event_id'].apply(lambda x: '_'.join(x.split('_')[0:2]))\n",
    "    data_test['event_idx'] = data_test.groupby('event_id', sort=False).ngroup()\n",
    "    \n",
    "    # Create time steps\n",
    "    data['event_t'] = data.groupby('event_id').cumcount()\n",
    "    data_test['event_t'] = data_test.groupby('event_id').cumcount()\n",
    "    \n",
    "    return data, data_test\n",
    "\n",
    "# Add model architecture classes here...\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    data, data_test = load_and_preprocess_data()\n",
    "    \n",
    "    print(\"Preprocessing time series...\")\n",
    "    preprocessor = TimeSeriesPreprocessor()\n",
    "    # Add preprocessing steps...\n",
    "    \n",
    "    print(\"Training model...\")\n",
    "    # Add training steps...\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
